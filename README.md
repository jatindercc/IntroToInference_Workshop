# 📘 Intro to Inference — Workshop

Welcome to the **Intro to Inference Workshop**!  
This hands-on learning experience introduces fundamental concepts of **statistical inference, probability distributions, and hypothesis testing**, blending **theory, visualization, and coding practice** in Python.

---

## 🚀 Workshop Overview

This workshop is designed for learners beginning their journey in **Data Science** and **Applied Statistics**. Through a mix of **Markdown explanations, interactive code, and visualization**, you will learn how to:

- Understand and calculate **Z-scores** and **T-scores**  
- Use **standardization** to transform datasets  
- Apply the **Shapiro-Wilk test** for testing **normality**  
- Connect **p-values** with hypothesis testing decisions  
- Normalize and standardize real-world datasets using **Min-Max scaling** and **z-score standardization**  
- Visualize statistical concepts with **histograms, density plots, and box plots**  
- Explore APIs and perform normality checks on **live datasets** from the web  
- Build coding confidence with both **scaffolded exercises** and **open-ended challenges**

---

## 📂 Repository Contents

- **`IntroToInference_Workshop.ipynb`**  
  The main notebook containing all explanations, exercises, and challenges.  
- **`data/`**  
  Sample datasets (e.g., `water.csv`) used in exercises.  
- **`images/`**  
  Illustrations and screenshots used to support the explanations.  

---

## 🛠️ Requirements

- Python 3.8+  
- Jupyter Notebook or JupyterLab  
- Required libraries:
  ```bash
  pip install pandas numpy matplotlib seaborn scipy scikit-learn requests

---

## 🌟 Key Challenges

- **Z-Score Challenge:** Calculate proportions and perform reverse lookups.  
- **T-Score Challenge:** Apply t-tests with degrees of freedom.  
- **Normality Testing:** Use Shapiro-Wilk to move beyond visual inspection.  
- **Climate Change Case Study:** Combine Z-scores, probabilities, and visualization.  
- **API Data Challenge:** Fetch and analyze country/NHL data, test for normality, normalize, and visualize.  

---

## 🚀 Why This Matters

Data Science is not only about coding but also about **trusting the numbers**.  
Without understanding concepts like **normality, p-values, and standardization**, we risk building models that:  

- Overvalue some features while ignoring others  
- Misinterpret randomness as meaningful signal  
- Produce misleading predictions in real-world decision making  

By mastering these inference tools, you’ll gain the ability to:  

- **Preprocess data correctly** so models treat all features fairly  
- **Test assumptions systematically**, rather than relying on “eyeballing” charts  
- **Explain results confidently**, making your analyses reproducible and credible  

👉 This matters because robust inference is what separates exploratory analysis from **responsible, evidence-based decision making** in Data Science.

---

## 🎯 Learning Outcomes

By the end of this workshop, you will be able to:

- Explain and apply Z-scores, T-scores, and p-values  
- Perform statistical tests for normality  
- Normalize and standardize datasets for machine learning workflows  
- Visualize statistical concepts for deeper understanding  
- Automate statistical checks with Python  
- Reflect critically on **why preprocessing matters** in Data Science  

---

## 👩‍🏫 Audience

This workshop is intended for:  
- Students learning **Data Science** or **Statistics**  
- Beginners in **Machine Learning** needing stronger statistical foundations  
- Practitioners seeking a refresher on **statistical inference concepts**  

---

## 📢 How to Use This Workshop

1. Clone or download this repository.  
2. Open the `IntroToInference_Workshop.ipynb` notebook in Jupyter.  
3. Work through the sections in order, running code and completing challenges.  
4. Use the `data/` and `images/` directories for reference.  
5. Reflect and extend exercises with your own datasets.

---
